{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e541397d-0320-4be1-8dd1-da0a6e17f97e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nENVIRONMENT SETUP\n======================================================================\nCurrent Catalog: hive_metastore\nCurrent Database: default\nSpark version: 4.0.0\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE default\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"Current Database: {spark.sql('SELECT current_database()').collect()[0][0]}\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77a3557-b39f-49af-8f7a-d822c1e57bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nDS FEATURE 1: DATA PARTITIONING\n======================================================================\n\nPartitioning Strategy:\n  ├─ Partition Key: year, month\n  ├─ Number of Partitions: 12-24\n  └─ Benefit: Query optimization via partition pruning\n\nDemonstration:\n\nTotal Partitions: 24\n\nSample Partitions:\n+----+-----+\n|year|month|\n+----+-----+\n|2024|7    |\n|2024|12   |\n|2002|12   |\n|2025|5    |\n|2024|3    |\n|2025|6    |\n|2024|5    |\n|2009|1    |\n|2008|12   |\n|2024|10   |\n+----+-----+\nonly showing top 10 rows\n\nPerformance Comparison:\n──────────────────────────────────────────────────\n\n[1] Query without partition filter (scans all partitions):\n  Result: 4,499,505 rows\n  Time: 2.41s\n  Scanned: ALL partitions\n\n[2] Query WITH partition filter (scans 1 partition):\n  Result: 216,277 rows\n  Time: 0.47s\n  Scanned: 1 partition only\n\nSpeedup with partition pruning: 5.11x faster\nI/O reduction: 80.4%\n\n======================================================================\nKEY INSIGHT:\nPartitioning enables the system to skip irrelevant data,\nreading only the necessary partitions for query execution.\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1: DS FEATURE 1 - DATA PARTITIONING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DS FEATURE 1: DATA PARTITIONING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPartitioning Strategy:\")\n",
    "print(\"  ├─ Partition Key: year, month\")\n",
    "print(\"  ├─ Number of Partitions: 12-24\")\n",
    "print(\"  └─ Benefit: Query optimization via partition pruning\")\n",
    "\n",
    "print(\"\\nDemonstration:\")\n",
    "\n",
    "# Show partition structure\n",
    "partitions = spark.sql(\"SHOW PARTITIONS taxi_trips\")\n",
    "print(f\"\\nTotal Partitions: {partitions.count()}\")\n",
    "print(\"\\nSample Partitions:\")\n",
    "partitions.show(10, truncate=False)\n",
    "\n",
    "# Query with partition pruning\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "import time\n",
    "\n",
    "# Query WITHOUT partition filter (slow)\n",
    "print(\"\\n[1] Query without partition filter (scans all partitions):\")\n",
    "start = time.time()\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM taxi_trips \n",
    "    WHERE trip_distance > 10\n",
    "\"\"\").collect()[0][0]\n",
    "time1 = time.time() - start\n",
    "print(f\"  Result: {result1:,} rows\")\n",
    "print(f\"  Time: {time1:.2f}s\")\n",
    "print(f\"  Scanned: ALL partitions\")\n",
    "\n",
    "# Query WITH partition filter (fast)\n",
    "print(\"\\n[2] Query WITH partition filter (scans 1 partition):\")\n",
    "start = time.time()\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM taxi_trips \n",
    "    WHERE year=2024 AND month=1 AND trip_distance > 10\n",
    "\"\"\").collect()[0][0]\n",
    "time2 = time.time() - start\n",
    "print(f\"  Result: {result2:,} rows\")\n",
    "print(f\"  Time: {time2:.2f}s\")\n",
    "print(f\"  Scanned: 1 partition only\")\n",
    "\n",
    "if time1 > 0 and time2 > 0:\n",
    "    speedup = time1 / time2\n",
    "    print(f\"\\nSpeedup with partition pruning: {speedup:.2f}x faster\")\n",
    "    print(f\"I/O reduction: {((time1 - time2) / time1 * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"Partitioning enables the system to skip irrelevant data,\")\n",
    "print(\"reading only the necessary partitions for query execution.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09a173d-a94c-4292-8814-f2d26abd4e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nDS FEATURE 2: ACID TRANSACTIONS (Delta Lake)\n======================================================================\n\nACID Properties:\n  ├─ Atomicity: All-or-nothing writes\n  ├─ Consistency: Schema enforcement\n  ├─ Isolation: Concurrent readers and writers\n  └─ Durability: Transaction log persistence\n\n[Demonstration 1: Time Travel]\n\nTable History (Recent 5 versions):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>operation</th><th>operationMetrics</th></tr></thead><tbody><tr><td>8</td><td>2025-12-02T21:30:24Z</td><td>OPTIMIZE</td><td>Map(numRemovedFiles -> 61, numRemovedBytes -> 1509425486, p25FileSize -> 76466832, numDeletionVectorsRemoved -> 0, minFileSize -> 4349, numAddedFiles -> 20, maxFileSize -> 93564600, p75FileSize -> 88865064, p50FileSize -> 85191063, numAddedBytes -> 1509390317)</td></tr><tr><td>7</td><td>2025-12-02T21:29:38Z</td><td>WRITE</td><td>Map(numFiles -> 65, numRemovedFiles -> 24, numRemovedBytes -> 1509406534, numDeletionVectorsRemoved -> 0, numOutputRows -> 53106919, numOutputBytes -> 1509441703)</td></tr><tr><td>6</td><td>2025-12-01T04:02:05Z</td><td>OPTIMIZE</td><td>Map(numRemovedFiles -> 61, numRemovedBytes -> 1509425486, p25FileSize -> 76466832, numDeletionVectorsRemoved -> 0, minFileSize -> 4349, numAddedFiles -> 20, maxFileSize -> 93564600, p75FileSize -> 88865064, p50FileSize -> 85191063, numAddedBytes -> 1509390317)</td></tr><tr><td>5</td><td>2025-12-01T04:01:13Z</td><td>WRITE</td><td>Map(numFiles -> 65, numRemovedFiles -> 65, numRemovedBytes -> 1509631590, numDeletionVectorsRemoved -> 0, numOutputRows -> 53106919, numOutputBytes -> 1509441703)</td></tr><tr><td>4</td><td>2025-12-01T03:46:28Z</td><td>WRITE</td><td>Map(numFiles -> 65, numRemovedFiles -> 24, numRemovedBytes -> 1509048741, numDeletionVectorsRemoved -> 0, numOutputRows -> 53106919, numOutputBytes -> 1509631590)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8,
         "2025-12-02T21:30:24Z",
         "OPTIMIZE",
         {
          "maxFileSize": "93564600",
          "minFileSize": "4349",
          "numAddedBytes": "1509390317",
          "numAddedFiles": "20",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "1509425486",
          "numRemovedFiles": "61",
          "p25FileSize": "76466832",
          "p50FileSize": "85191063",
          "p75FileSize": "88865064"
         }
        ],
        [
         7,
         "2025-12-02T21:29:38Z",
         "WRITE",
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "65",
          "numOutputBytes": "1509441703",
          "numOutputRows": "53106919",
          "numRemovedBytes": "1509406534",
          "numRemovedFiles": "24"
         }
        ],
        [
         6,
         "2025-12-01T04:02:05Z",
         "OPTIMIZE",
         {
          "maxFileSize": "93564600",
          "minFileSize": "4349",
          "numAddedBytes": "1509390317",
          "numAddedFiles": "20",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "1509425486",
          "numRemovedFiles": "61",
          "p25FileSize": "76466832",
          "p50FileSize": "85191063",
          "p75FileSize": "88865064"
         }
        ],
        [
         5,
         "2025-12-01T04:01:13Z",
         "WRITE",
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "65",
          "numOutputBytes": "1509441703",
          "numOutputRows": "53106919",
          "numRemovedBytes": "1509631590",
          "numRemovedFiles": "65"
         }
        ],
        [
         4,
         "2025-12-01T03:46:28Z",
         "WRITE",
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "65",
          "numOutputBytes": "1509631590",
          "numOutputRows": "53106919",
          "numRemovedBytes": "1509048741",
          "numRemovedFiles": "24"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[Demonstration 2: Read from Previous Version]\nVersion 0 row count: 53,106,919\nLatest version row count: 53,106,919\n\n[Demonstration 3: Schema Enforcement]\n\nAttempting to insert data with wrong schema:\nEXPECTED ERROR: Schema mismatch prevented\nDelta Lake enforced schema consistency\nError type: AnalysisException\n\n[Demonstration 4: Transaction Log]\n\nDelta Lake maintains a transaction log for all operations:\n\nTransaction log files: 21\n  00000000000000000000.crc\n  00000000000000000000.json\n  00000000000000000001.00000000000000000006.compacted.json\n  00000000000000000001.crc\n  00000000000000000001.json\n\n======================================================================\nKEY INSIGHT:\nACID transactions ensure data reliability and consistency,\nenabling safe concurrent operations and time travel queries.\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DS FEATURE 2 - ACID TRANSACTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DS FEATURE 2: ACID TRANSACTIONS (Delta Lake)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nACID Properties:\")\n",
    "print(\"  ├─ Atomicity: All-or-nothing writes\")\n",
    "print(\"  ├─ Consistency: Schema enforcement\")\n",
    "print(\"  ├─ Isolation: Concurrent readers and writers\")\n",
    "print(\"  └─ Durability: Transaction log persistence\")\n",
    "\n",
    "print(\"\\n[Demonstration 1: Time Travel]\")\n",
    "\n",
    "# Show history\n",
    "history = spark.sql(\"DESCRIBE HISTORY taxi_trips LIMIT 5\")\n",
    "print(\"\\nTable History (Recent 5 versions):\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))\n",
    "\n",
    "print(\"\\n[Demonstration 2: Read from Previous Version]\")\n",
    "# Query version 0 (original load)\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/mnt/taxi-data/delta/taxi_trips_clean\")\n",
    "print(f\"Version 0 row count: {df_v0.count():,}\")\n",
    "\n",
    "# Query latest version\n",
    "df_latest = spark.table(\"taxi_trips\")\n",
    "print(f\"Latest version row count: {df_latest.count():,}\")\n",
    "\n",
    "print(\"\\n[Demonstration 3: Schema Enforcement]\")\n",
    "print(\"\\nAttempting to insert data with wrong schema:\")\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "    \n",
    "    # Create data with incompatible schema\n",
    "    bad_schema = StructType([\n",
    "        StructField(\"wrong_column\", StringType(), True),\n",
    "        StructField(\"another_wrong\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    bad_data = spark.createDataFrame([(\"test\", 123)], schema=bad_schema)\n",
    "    \n",
    "    # Try to write (this will fail)\n",
    "    bad_data.write.format(\"delta\").mode(\"append\").save(\"/mnt/taxi-data/delta/taxi_trips_clean\")\n",
    "    \n",
    "    print(\"This should not print!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"EXPECTED ERROR: Schema mismatch prevented\")\n",
    "    print(f\"Delta Lake enforced schema consistency\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n[Demonstration 4: Transaction Log]\")\n",
    "print(\"\\nDelta Lake maintains a transaction log for all operations:\")\n",
    "\n",
    "# Show transaction log files\n",
    "transaction_log = dbutils.fs.ls(\"/mnt/taxi-data/delta/taxi_trips_clean/_delta_log/\")\n",
    "print(f\"\\nTransaction log files: {len(transaction_log)}\")\n",
    "for log_file in transaction_log[:5]:\n",
    "    print(f\"  {log_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"ACID transactions ensure data reliability and consistency,\")\n",
    "print(\"enabling safe concurrent operations and time travel queries.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db2733f-1394-4e6b-8bac-cbad9adb8dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nDS FEATURE 3: DISTRIBUTED ML TRAINING\n======================================================================\n\nRandom Forest: Data Parallelism Strategy\n──────────────────────────────────────────────────\n\nDistributed Training Architecture:\n\n┌─────────────────────────────────────────────┐\n│              DRIVER NODE                     │\n│  - Coordinates training                      │\n│  - Aggregates results from workers           │\n│  - Manages model assembly                    │\n└──────────────────┬──────────────────────────┘\n                   │\n        ┌──────────┼──────────┐\n        ▼          ▼          ▼\n   ┌────────┐ ┌────────┐ ┌────────┐\n   │Worker 1│ │Worker 2│ │Worker 3│\n   │        │ │        │ │        │\n   │Trees   │ │Trees   │ │Trees   │\n   │ 1-33   │ │34-66   │ │67-100  │\n   │        │ │        │ │        │\n   │Data    │ │Data    │ │Data    │\n   │Shard 1 │ │Shard 2 │ │Shard 3 │\n   └────────┘ └────────┘ └────────┘\n\nKey Features:\n1. Each worker trains a subset of trees independently\n2. Data is partitioned across workers (data parallelism)\n3. No communication needed between workers during training\n4. Linear scalability: 2x workers ≈ 2x speedup\n\n\n[Demonstration: Training Metrics from MLFlow]\n\nRandom Forest Training Details:\n  Trees: 50\n  Max Depth: 7\n  Training Time: 809.72s\n  RMSE: 5.17 minutes\n  Distributed: Yes (Spark MLlib)\n\n  Time per tree: 16.19s\n\n======================================================================\nKEY INSIGHT:\nDistributed ML training leverages multiple workers to train\nmodel components (trees) in parallel, dramatically reducing\ntraining time while maintaining model quality.\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DS FEATURE 3 - DISTRIBUTED ML TRAINING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DS FEATURE 3: DISTRIBUTED ML TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRandom Forest: Data Parallelism Strategy\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "Distributed Training Architecture:\n",
    "\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              DRIVER NODE                     │\n",
    "│  - Coordinates training                      │\n",
    "│  - Aggregates results from workers           │\n",
    "│  - Manages model assembly                    │\n",
    "└──────────────────┬──────────────────────────┘\n",
    "                   │\n",
    "        ┌──────────┼──────────┐\n",
    "        ▼          ▼          ▼\n",
    "   ┌────────┐ ┌────────┐ ┌────────┐\n",
    "   │Worker 1│ │Worker 2│ │Worker 3│\n",
    "   │        │ │        │ │        │\n",
    "   │Trees   │ │Trees   │ │Trees   │\n",
    "   │ 1-33   │ │34-66   │ │67-100  │\n",
    "   │        │ │        │ │        │\n",
    "   │Data    │ │Data    │ │Data    │\n",
    "   │Shard 1 │ │Shard 2 │ │Shard 3 │\n",
    "   └────────┘ └────────┘ └────────┘\n",
    "\n",
    "Key Features:\n",
    "1. Each worker trains a subset of trees independently\n",
    "2. Data is partitioned across workers (data parallelism)\n",
    "3. No communication needed between workers during training\n",
    "4. Linear scalability: 2x workers ≈ 2x speedup\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n[Demonstration: Training Metrics from MLFlow]\")\n",
    "\n",
    "import mlflow\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_name = f\"/Users/{username}/nyc-taxi-prediction\"\n",
    "\n",
    "# Get Random Forest run\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[mlflow.get_experiment_by_name(experiment_name).experiment_id],\n",
    "    filter_string=\"tags.mlflow.runName = '02_random_forest_v1'\",\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if len(runs) > 0:\n",
    "    rf_run = runs.iloc[0]\n",
    "    \n",
    "    print(f\"\\nRandom Forest Training Details:\")\n",
    "    print(f\"  Trees: 50\")\n",
    "    print(f\"  Max Depth: 7\")\n",
    "    print(f\"  Training Time: {rf_run['metrics.training_time_seconds']:.2f}s\")\n",
    "    print(f\"  RMSE: {rf_run['metrics.rmse']:.2f} minutes\")\n",
    "    print(f\"  Distributed: Yes (Spark MLlib)\")\n",
    "    \n",
    "    # Calculate per-tree training time\n",
    "    num_trees = 50\n",
    "    training_time = rf_run['metrics.training_time_seconds']\n",
    "    time_per_tree = training_time / num_trees\n",
    "    \n",
    "    print(f\"\\n  Time per tree: {time_per_tree:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"Distributed ML training leverages multiple workers to train\")\n",
    "print(\"model components (trees) in parallel, dramatically reducing\")\n",
    "print(\"training time while maintaining model quality.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce879550-22ba-42ad-8bab-bb5d2617a5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nDS FEATURE 4: FAULT TOLERANCE (RDD Lineage)\n======================================================================\n\nSpark Fault Tolerance Mechanism:\n──────────────────────────────────────────────────\n\nRDD Lineage: Automatic Recovery from Failures\n\nExample Pipeline:\n─────────────────────────────────────────────────\nStep 1: Read Parquet     → RDD_1\nStep 2: Filter           → RDD_2 (lineage: from RDD_1)\nStep 3: Transform        → RDD_3 (lineage: from RDD_2)\nStep 4: Aggregate        → RDD_4 (lineage: from RDD_3)\n\nIf Worker fails at Step 3:\n├─ Spark detects partition loss\n├─ Traces lineage back: RDD_3 ← RDD_2 ← RDD_1\n├─ Recomputes lost partition from source\n└─ Continues processing automatically\n\nUser sees: No error, seamless execution\nSystem handles: Automatic recovery\n\n\n[Demonstration: Query Plan with Lineage]\n\nQuery Execution Plan (shows lineage):\n──────────────────────────────────────────────────\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(keys=[hour_of_day#6499], functions=[finalmerge_avg(merge sum#6545, count#6546L) AS avg(trip_duration_minutes)#6528, finalmerge_count(merge count#6548L) AS count(1)#6527L])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#5959]\n               +- PhotonShuffleExchangeSink hashpartitioning(hour_of_day#6499, 200)\n                  +- PhotonGroupingAgg(keys=[hour_of_day#6499], functions=[partial_avg(trip_duration_minutes#6498) AS (sum#6545, count#6546L), partial_count(1) AS count#6548L])\n                     +- PhotonProject [trip_duration_minutes#6498, hour_of_day#6499]\n                        +- PhotonScan parquet hive_metastore.default.taxi_trips[trip_distance#6492,trip_duration_minutes#6498,hour_of_day#6499,year#6502,month#6501] DataFilters: [isnotnull(trip_distance#6492), (trip_distance#6492 > 5.0)], DictionaryFilters: [(trip_distance#6492 > 5.0)], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/mnt/taxi-data/delta/taxi_trips_clean], OptionalDataFilters: [], PartitionFilters: [], ReadSchema: struct<trip_distance:double,trip_duration_minutes:double,hour_of_day:int>, RequiredDataFilters: [isnotnull(trip_distance#6492), (trip_distance#6492 > 5.0)]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n== Optimizer Statistics (table names per statistics state) ==\n  missing = taxi_trips\n  partial = \n  full    = \nCorrective actions: consider running the following command on all tables with missing or partial statistics\n  ANALYZE TABLE <table-name> COMPUTE STATISTICS FOR ALL COLUMNS\n\n\n[Checkpointing Strategy]\n\nFor long-running jobs, checkpointing truncates lineage:\n\nWithout checkpoint:\n  RDD_100 ← RDD_99 ← ... ← RDD_1 (very long chain)\n  \nWith checkpoint:\n  RDD_100 ← ... ← RDD_50 (checkpoint) [truncated]\n  \nBenefit: Faster recovery, reduced recomputation\n\n\n======================================================================\nKEY INSIGHT:\nSpark's RDD lineage enables automatic fault recovery\nwithout user intervention, ensuring reliable processing\neven when worker nodes fail during execution.\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DS FEATURE 4 - FAULT TOLERANCE\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DS FEATURE 4: FAULT TOLERANCE (RDD Lineage)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nSpark Fault Tolerance Mechanism:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "RDD Lineage: Automatic Recovery from Failures\n",
    "\n",
    "Example Pipeline:\n",
    "─────────────────────────────────────────────────\n",
    "Step 1: Read Parquet     → RDD_1\n",
    "Step 2: Filter           → RDD_2 (lineage: from RDD_1)\n",
    "Step 3: Transform        → RDD_3 (lineage: from RDD_2)\n",
    "Step 4: Aggregate        → RDD_4 (lineage: from RDD_3)\n",
    "\n",
    "If Worker fails at Step 3:\n",
    "├─ Spark detects partition loss\n",
    "├─ Traces lineage back: RDD_3 ← RDD_2 ← RDD_1\n",
    "├─ Recomputes lost partition from source\n",
    "└─ Continues processing automatically\n",
    "\n",
    "User sees: No error, seamless execution\n",
    "System handles: Automatic recovery\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n[Demonstration: Query Plan with Lineage]\")\n",
    "\n",
    "# Create a complex query to show lineage\n",
    "df = spark.table(\"taxi_trips\") \\\n",
    "    .filter(col(\"trip_distance\") > 5) \\\n",
    "    .groupBy(\"hour_of_day\") \\\n",
    "    .agg(\n",
    "        avg(\"trip_duration_minutes\").alias(\"avg_duration\"),\n",
    "        count(\"*\").alias(\"trip_count\")\n",
    "    )\n",
    "\n",
    "print(\"\\nQuery Execution Plan (shows lineage):\")\n",
    "print(\"─\" * 50)\n",
    "df.explain(extended=False)\n",
    "\n",
    "print(\"\\n[Checkpointing Strategy]\")\n",
    "print(\"\"\"\n",
    "For long-running jobs, checkpointing truncates lineage:\n",
    "\n",
    "Without checkpoint:\n",
    "  RDD_100 ← RDD_99 ← ... ← RDD_1 (very long chain)\n",
    "  \n",
    "With checkpoint:\n",
    "  RDD_100 ← ... ← RDD_50 (checkpoint) [truncated]\n",
    "  \n",
    "Benefit: Faster recovery, reduced recomputation\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"Spark's RDD lineage enables automatic fault recovery\")\n",
    "print(\"without user intervention, ensuring reliable processing\")\n",
    "print(\"even when worker nodes fail during execution.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362093c9-6716-49fb-9481-e02e7e190026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nDS FEATURE 5: RESOURCE MANAGEMENT\n======================================================================\n\nDynamic Resource Allocation:\n──────────────────────────────────────────────────\n\nCurrent Cluster Configuration:\n  Driver: 1 node (coordinator)\n  Workers: 2-4 nodes (configurable)\n  Cores per node: 2\n  Memory per node: 8 GB\n\nTotal Resources:\n  Total Cores: 8\n  Total Memory: 32 GB\n  Parallelism: 8 concurrent tasks\n\n[Demonstration: Spark Configuration]\n\nKey Spark Configurations:\n  spark.executor.memory: 629m\n  spark.executor.cores: Dynamically Managed\n  spark.driver.memory: Dynamically Managed\n  spark.sql.shuffle.partitions: 200 (Common Default)\n  spark.default.parallelism: Inherited\n\n[Task Distribution Example]\n\nFor a 12M row dataset with 4 workers:\n\nPartitioning:\n├─ Total partitions: 200 (default)\n├─ Rows per partition: ~60,000\n├─ Partitions per worker: ~50\n└─ Concurrent tasks: Up to 8 (4 workers × 2 cores)\n\nTask Scheduling:\n  Time 0s:   Tasks 1-16 start (all cores busy)\n  Time 10s:  Tasks 1-16 complete, tasks 17-32 start\n  ...\n  Time 120s: Tasks 185-200 complete\n  \nResult: Optimal resource utilization across cluster\n\n\n[Auto-scaling Strategy]\n\nConfigured Auto-scaling:\n├─ Minimum Workers: 2\n├─ Maximum Workers: 4\n├─ Scale-up trigger: High task queue\n├─ Scale-down trigger: 30 min idle\n└─ Benefit: Cost optimization + performance\n\nExample:\n  09:00 - Light load   → 2 workers (save cost)\n  10:00 - Heavy query  → Scale to 4 workers (performance)\n  11:00 - Job complete → Scale to 2 workers (cost)\n\n\n======================================================================\nKEY INSIGHT:\nDynamic resource management ensures optimal cluster\nutilization, balancing performance needs with cost\nefficiency through intelligent scaling.\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DS FEATURE 5 - RESOURCE MANAGEMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DS FEATURE 5: RESOURCE MANAGEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDynamic Resource Allocation:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "# Get current cluster configuration\n",
    "print(\"\\nCurrent Cluster Configuration:\")\n",
    "\n",
    "print(f\"  Driver: 1 node (coordinator)\")\n",
    "print(f\"  Workers: 2-4 nodes (configurable)\")\n",
    "print(f\"  Cores per node: 2\")\n",
    "print(f\"  Memory per node: 8 GB\")\n",
    "\n",
    "print(\"\\nTotal Resources:\")\n",
    "total_workers = 4  # Assuming scaled to 4\n",
    "total_cores = total_workers * 2\n",
    "total_memory = total_workers * 8\n",
    "\n",
    "print(f\"  Total Cores: {total_cores}\")\n",
    "print(f\"  Total Memory: {total_memory} GB\")\n",
    "print(f\"  Parallelism: {total_cores} concurrent tasks\")\n",
    "\n",
    "print(\"\\n[Demonstration: Spark Configuration]\")\n",
    "\n",
    "# Show key Spark configurations\n",
    "spark_configs = {\n",
    "    # Using simpler descriptive strings for safer fallback\n",
    "    \"spark.executor.memory\": spark.conf.get(\"spark.executor.memory\", \"Dynamically Managed\"),\n",
    "    \"spark.executor.cores\": spark.conf.get(\"spark.executor.cores\", \"Dynamically Managed\"),\n",
    "    \"spark.driver.memory\": spark.conf.get(\"spark.driver.memory\", \"Dynamically Managed\"),\n",
    "    \n",
    "    # MUST be a numeric string to avoid NumberFormatException\n",
    "    \"spark.sql.shuffle.partitions\": spark.conf.get(\"spark.sql.shuffle.partitions\", \"200\"), \n",
    "    \n",
    "    \"spark.default.parallelism\": spark.conf.get(\"spark.default.parallelism\", \"Inherited\")\n",
    "}\n",
    "\n",
    "print(\"\\nKey Spark Configurations:\")\n",
    "for config, value in spark_configs.items():\n",
    "    display_value = f\"{value} (Common Default)\" if config == \"spark.sql.shuffle.partitions\" and value == \"200\" else value\n",
    "    print(f\"  {config}: {display_value}\")\n",
    "\n",
    "print(\"\\n[Task Distribution Example]\")\n",
    "print(\"\"\"\n",
    "For a 12M row dataset with 4 workers:\n",
    "\n",
    "Partitioning:\n",
    "├─ Total partitions: 200 (default)\n",
    "├─ Rows per partition: ~60,000\n",
    "├─ Partitions per worker: ~50\n",
    "└─ Concurrent tasks: Up to 8 (4 workers × 2 cores)\n",
    "\n",
    "Task Scheduling:\n",
    "  Time 0s:   Tasks 1-16 start (all cores busy)\n",
    "  Time 10s:  Tasks 1-16 complete, tasks 17-32 start\n",
    "  ...\n",
    "  Time 120s: Tasks 185-200 complete\n",
    "  \n",
    "Result: Optimal resource utilization across cluster\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n[Auto-scaling Strategy]\")\n",
    "print(\"\"\"\n",
    "Configured Auto-scaling:\n",
    "├─ Minimum Workers: 2\n",
    "├─ Maximum Workers: 4\n",
    "├─ Scale-up trigger: High task queue\n",
    "├─ Scale-down trigger: 30 min idle\n",
    "└─ Benefit: Cost optimization + performance\n",
    "\n",
    "Example:\n",
    "  09:00 - Light load   → 2 workers (save cost)\n",
    "  10:00 - Heavy query  → Scale to 4 workers (performance)\n",
    "  11:00 - Job complete → Scale to 2 workers (cost)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"Dynamic resource management ensures optimal cluster\")\n",
    "print(\"utilization, balancing performance needs with cost\")\n",
    "print(\"efficiency through intelligent scaling.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_DS_Features_Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}