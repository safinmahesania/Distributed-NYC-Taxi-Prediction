{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931028c3-bdc5-406c-84e3-67c3b6067dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ace0c6-1477-4a3e-8342-9af49b018be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nENVIRONMENT SETUP\n======================================================================\n✓ Current Catalog: hive_metastore\n✓ Current Database: default\n✓ Spark version: 4.0.0\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE default\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"✓ Current Database: {spark.sql('SELECT current_database()').collect()[0][0]}\")\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a21c27f-650e-44ec-918a-873046b119dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 53,106,919 rows\n\nTraining set: 42,487,529 rows\nTest set: 10,619,390 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load Data and Split\n",
    "# ============================================\n",
    "\n",
    "# Load the clean data\n",
    "df = spark.table(\"taxi_trips\")\n",
    "\n",
    "# Verify data\n",
    "print(f\"Loaded {df.count():,} rows\")\n",
    "\n",
    "# Create train/test split (80/20)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache for performance\n",
    "train.cache()\n",
    "test.cache()\n",
    "\n",
    "print(f\"\\nTraining set: {train.count():,} rows\")\n",
    "print(f\"Test set: {test.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddadcdcf-80b2-4fa7-9e29-d07d37e2ce3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNumeric features: 6\nCategorical features: 2\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Define Feature Columns\n",
    "# ============================================\n",
    "\n",
    "# Numeric features\n",
    "numeric_features = [\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\",\n",
    "    \"fare_amount\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\"\n",
    "]\n",
    "\n",
    "# Categorical features (location IDs)\n",
    "categorical_features = [\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\"\n",
    "]\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dd03de-655c-477c-a18a-a7e07e9d584c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBuilding feature preprocessing pipeline...\nPipeline stages:\n  1. StringIndexer\n  2. StringIndexer\n  3. VectorAssembler\n  4. StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Feature Preprocessing Pipeline\n",
    "# DS FEATURE: Distributed feature transformation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nBuilding feature preprocessing pipeline...\")\n",
    "\n",
    "# Stage 1: Index categorical features\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=col, \n",
    "        outputCol=f\"{col}_indexed\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for col in categorical_features\n",
    "]\n",
    "\n",
    "# Stage 2: Assemble all features\n",
    "indexed_categorical = [f\"{col}_indexed\" for col in categorical_features]\n",
    "all_feature_cols = numeric_features + indexed_categorical\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Stage 3: Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "preprocessing_pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
    "\n",
    "print(\"Pipeline stages:\")\n",
    "for i, stage in enumerate(preprocessing_pipeline.getStages()):\n",
    "    print(f\"  {i+1}. {stage.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f69b870-b3bd-4c38-a8ff-c54d21648a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFitting preprocessing pipeline...\nPipeline fitted and applied in 47.23 seconds\n\nTraining samples: 42,487,529\nTest samples: 10,619,390\nFeature vector size: 8\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Fit Pipeline and Transform Data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFitting preprocessing pipeline...\")\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit on training data\n",
    "pipeline_model = preprocessing_pipeline.fit(train)\n",
    "\n",
    "# Transform both train and test\n",
    "train_processed = pipeline_model.transform(train)\n",
    "test_processed = pipeline_model.transform(test)\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pipeline fitted and applied in {processing_time:.2f} seconds\")\n",
    "\n",
    "# Select final columns for ML\n",
    "final_cols = [\"features\", \"trip_duration_minutes\"]\n",
    "\n",
    "train_ml = train_processed.select(final_cols)\n",
    "test_ml = test_processed.select(final_cols)\n",
    "\n",
    "# Cache processed data\n",
    "train_ml.cache()\n",
    "test_ml.cache()\n",
    "\n",
    "print(f\"\\nTraining samples: {train_ml.count():,}\")\n",
    "print(f\"Test samples: {test_ml.count():,}\")\n",
    "print(f\"Feature vector size: {len(all_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f58c2bc-bd32-402f-9516-0237cbf6c708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSaving ML-ready datasets...\nML datasets saved:\n  - taxi_ml_train\n  - taxi_ml_test\n  - Preprocessing pipeline saved\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Save ML-Ready Data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nSaving ML-ready datasets...\")\n",
    "\n",
    "# Save as Delta tables\n",
    "train_ml.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"taxi_ml_train\")\n",
    "test_ml.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"taxi_ml_test\")\n",
    "\n",
    "# Save preprocessing model\n",
    "pipeline_model.write().overwrite().save(\"/mnt/taxi-data/models/preprocessing_pipeline\")\n",
    "\n",
    "print(\"ML datasets saved:\")\n",
    "print(\"  - taxi_ml_train\")\n",
    "print(\"  - taxi_ml_test\")\n",
    "print(\"  - Preprocessing pipeline saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a80e97-faa5-4169-ba20-7989615d02d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nML DATA VERIFICATION\n============================================================\n\nSample ML data:\n+---------------------------------------------------------------------------------------------------------------------------+---------------------+\n|features                                                                                                                   |trip_duration_minutes|\n+---------------------------------------------------------------------------------------------------------------------------+---------------------+\n|[1.0899394519394712,1.2792148626010864,2.4495977928807227,0.0,3.0709646716449024,0.0,2.3271751888860606,0.6753788003241491]|55.733333333333334   |\n|[0.22243662284479002,1.2792148626010864,0.6798195709600171,0.0,3.0709646716449024,0.0,1.606859058992756,1.2796650953510194]|13.983333333333333   |\n|[0.378142258836143,1.2792148626010864,0.7191479758915884,0.0,3.0709646716449024,0.0,0.609498263755873,0.03554625264863943] |10.083333333333334   |\n|[0.5338478948274961,1.2792148626010864,1.309074049865157,0.0,3.0709646716449024,0.0,0.7757250629620202,1.0308413268105434] |35.13333333333333    |\n|[1.445838048491135,1.2792148626010864,1.820343313975583,0.0,3.0709646716449024,0.0,1.0527697283055988,1.3863038532969376]  |25.883333333333333   |\n+---------------------------------------------------------------------------------------------------------------------------+---------------------+\nonly showing top 5 rows\n\nFeature vector size: 8\nTarget variable: 55.73 minutes\n\n============================================================\nML DATA PREPARATION COMPLETE\nReady for model training\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 6: Verify ML Data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample ML data:\")\n",
    "train_ml.show(5, truncate=False)\n",
    "\n",
    "# Feature vector inspection\n",
    "sample_row = train_ml.first()\n",
    "print(f\"\\nFeature vector size: {len(sample_row['features'])}\")\n",
    "print(f\"Target variable: {sample_row['trip_duration_minutes']:.2f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML DATA PREPARATION COMPLETE\")\n",
    "print(\"Ready for model training\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ML_Data_Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}